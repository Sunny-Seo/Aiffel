{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9997f1",
   "metadata": {},
   "source": [
    "# 4. 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9098d",
   "metadata": {},
   "source": [
    "### 학습과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf21fc",
   "metadata": {},
   "source": [
    "1. 데이터 다운로드\n",
    "2. 데이터 읽어오기\n",
    "3. 데이터 정제\n",
    "4. 평가 데이터셋 분리\n",
    "5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2434bba",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6a5cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list=  glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "#여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()  # 텍스트를 라인 단위로 끊어서 읽어오기\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print('데이터의 크기:', len(raw_corpus))\n",
    "print('Examples:\\n', raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350a9c4",
   "metadata": {},
   "source": [
    "### 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff015430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n",
      "You saw her bathing on the roof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   #길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[0] == '[': continue    # 문장 처음이 '['인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == ']': continue   # 문장 끝이 ']'d인 문장은 건너뜁니다.\n",
    "    if idx > 10: break   #문장 10개만 확인\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dea331",
   "metadata": {},
   "source": [
    "토큰화\n",
    "\n",
    "    1. 문장 부호 양쪽에 공백을 추가\n",
    "    2. 모든 문자들을 소문자로 변환\n",
    "    3. 특수문자들은 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d5a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()    # 소문자로 바꾸고 양족 공백을 삭제\n",
    "    \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)     # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)   # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    sentence = '<start> ' + sentence + ' <end>'       # 문자 앞뒤로 <start>와 <end> 를 단어처럼 붙여줌\n",
    "    \n",
    "    if \"verse\" in sentence:\n",
    "        sentence = sentence.replace(\"verse\", \"\")\n",
    "    if \"chorus\" in sentence:\n",
    "        sentence = sentence.replace(\"chorus\", \"\")\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ef95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    a0 = preprocess_sentence(sentence)\n",
    "    if len(a0.split()) > 15: continue\n",
    "    corpus.append(a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f393b387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156262\n",
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2971 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  117 ...    0    0    0]\n",
      " [   2  257  195 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f1774194c40>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있음. 이번에는 사용하지 안함\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 <unk> 처리\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 구축한 corpus로부터 Tokenizer가 사전을 자동구축\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환.\n",
    "   \n",
    "    for num in tensor:\n",
    "        if len(num) >= 29:\n",
    "            tensor = np.delete(tensor, num)\n",
    "            \n",
    "    print(len(tensor))\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "    \n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "464b658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 어떻게 생겼는지 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9179db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성, 마지막 토근은 <end> 가 아니라 <pad> 일 가능성이 높다.\n",
    "tgt_input = tensor[:, 1:]   # tensor에서 <start>를 잘라내서 타켓 문장을 생성.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "480ca90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                         tgt_input,\n",
    "                                                         test_size=0.2, \n",
    "                                                         shuffle=True,\n",
    "                                                         random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bf57a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (125009, 14)\n",
      "Target Train: (125009, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce743161",
   "metadata": {},
   "source": [
    "-> 학습데이터 갯수가 124960개 이하이므로 이대로 학습을 시켜주겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc001850",
   "metadata": {},
   "source": [
    "## 평가 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ac53ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_spoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 15000개와, 여기 포함되지 않은 0:<pad>를 포함하여 15001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d83b9",
   "metadata": {},
   "source": [
    "## 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "398c7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
    "    \n",
    "    for i in range(len(ax)):\n",
    "        ax[i].set_xlabel('Epochs')\n",
    "        ax[i].set_ylabel('Value')\n",
    "        \n",
    "        for n in range(len(list_of_metrics)):\n",
    "            if i == 0:\n",
    "                y = hist[list_of_metrics[n]]\n",
    "                if n == 0:\n",
    "                    ax[i].plot(epochs, y, label=\"train\")\n",
    "                else:\n",
    "                    ax[i].plot(epochs, y, label=\"val\")\n",
    "                ax[i].set_title('Loss')\n",
    "                ax[i].legend(loc='upper right')\n",
    "                if n == 1:\n",
    "                    break\n",
    "            else:\n",
    "                if n >= 2:\n",
    "                    y = hist[list_of_metrics[n]]\n",
    "                    if n == 2:\n",
    "                        ax[i].plot(epochs, y, label=\"train\")\n",
    "                    else:\n",
    "                        ax[i].plot(epochs, y, label=\"val\")\n",
    "                    ax[i].set_title('Accuracy')\n",
    "                    ax[i].legend(loc='lower right')\n",
    "                    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04a3b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.drop  = tf.keras.layers.Dropout(0.5)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6c674b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 7001), dtype=float32, numpy=\n",
       "array([[[ 1.17548836e-04, -1.14807386e-04, -2.63128841e-05, ...,\n",
       "          5.40828514e-05, -5.65763075e-05, -3.03391222e-04],\n",
       "        [ 1.21390425e-04, -2.94502010e-04, -1.28675325e-04, ...,\n",
       "         -3.02042622e-06, -2.51828933e-05, -3.82232538e-04],\n",
       "        [ 3.08477960e-04, -5.95455640e-04, -8.50504694e-06, ...,\n",
       "         -1.31302702e-04, -1.09974164e-04, -3.41724663e-04],\n",
       "        ...,\n",
       "        [-1.24180893e-04,  7.18953786e-04,  2.08809227e-03, ...,\n",
       "         -3.94588482e-04,  3.98348260e-04,  1.20393455e-03],\n",
       "        [-1.10266512e-04,  1.18035299e-03,  2.39084777e-03, ...,\n",
       "         -5.96074970e-04,  6.03696099e-04,  1.23693503e-03],\n",
       "        [-1.20426543e-04,  1.64948497e-03,  2.62871967e-03, ...,\n",
       "         -8.19587614e-04,  8.13114690e-04,  1.24008663e-03]],\n",
       "\n",
       "       [[ 1.17548836e-04, -1.14807386e-04, -2.63128841e-05, ...,\n",
       "          5.40828514e-05, -5.65763075e-05, -3.03391222e-04],\n",
       "        [-9.13777694e-05, -3.72882903e-04, -2.51393911e-04, ...,\n",
       "          3.68257897e-05, -1.01142010e-04, -4.54849476e-04],\n",
       "        [-8.20976638e-05, -4.32630652e-04, -5.60618297e-04, ...,\n",
       "         -1.62300712e-05,  1.29356456e-04, -4.20864089e-04],\n",
       "        ...,\n",
       "        [ 2.64276110e-04,  7.65863515e-05, -2.22461720e-04, ...,\n",
       "          1.62212877e-04, -1.92746360e-04,  6.88044645e-04],\n",
       "        [ 2.70819699e-04,  3.04625952e-04,  2.96357728e-04, ...,\n",
       "          5.81174645e-05, -1.33827372e-04,  9.51245718e-04],\n",
       "        [ 2.68731732e-04,  6.23246247e-04,  7.86670425e-04, ...,\n",
       "         -1.29061344e-04,  2.51447341e-06,  1.11753435e-03]],\n",
       "\n",
       "       [[ 1.17548836e-04, -1.14807386e-04, -2.63128841e-05, ...,\n",
       "          5.40828514e-05, -5.65763075e-05, -3.03391222e-04],\n",
       "        [-8.22625589e-05, -1.61391115e-04, -3.98095872e-04, ...,\n",
       "          1.03457482e-04, -4.42018318e-05, -7.26569560e-04],\n",
       "        [-2.68757489e-04,  1.08352891e-04, -1.61438627e-04, ...,\n",
       "          1.31194509e-04, -3.18837439e-04, -9.94956470e-04],\n",
       "        ...,\n",
       "        [-3.55207070e-04,  6.13526616e-04, -4.31629509e-04, ...,\n",
       "          2.74460937e-04,  4.12861875e-04, -8.43919755e-04],\n",
       "        [-4.75760346e-04,  4.70794010e-04, -4.76276218e-05, ...,\n",
       "          4.96147724e-04,  3.54028132e-04, -4.22965590e-04],\n",
       "        [-5.02872048e-04,  4.66276018e-04,  4.67652513e-04, ...,\n",
       "          5.51287085e-04,  3.45497916e-04, -4.61377495e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.17548836e-04, -1.14807386e-04, -2.63128841e-05, ...,\n",
       "          5.40828514e-05, -5.65763075e-05, -3.03391222e-04],\n",
       "        [ 1.76014131e-04, -2.97702463e-05,  1.52911409e-04, ...,\n",
       "          4.66521269e-05,  4.04804996e-05, -3.10580857e-04],\n",
       "        [ 3.43189255e-04, -9.56232689e-05,  4.07364598e-04, ...,\n",
       "         -1.08946153e-06,  3.83524137e-04, -4.01817961e-04],\n",
       "        ...,\n",
       "        [ 7.53989007e-05,  2.87297036e-04,  2.07808823e-03, ...,\n",
       "          1.74463465e-04, -1.49332089e-04, -7.55468791e-04],\n",
       "        [ 6.64878680e-05,  7.51684594e-04,  2.41927360e-03, ...,\n",
       "         -6.12293443e-05,  4.36497867e-05, -4.87385987e-04],\n",
       "        [ 3.73010953e-05,  1.24655932e-03,  2.69203470e-03, ...,\n",
       "         -3.23163724e-04,  2.75374012e-04, -2.63455528e-04]],\n",
       "\n",
       "       [[ 1.17548836e-04, -1.14807386e-04, -2.63128841e-05, ...,\n",
       "          5.40828514e-05, -5.65763075e-05, -3.03391222e-04],\n",
       "        [-4.69819606e-05, -3.06055415e-04,  1.39125332e-04, ...,\n",
       "          5.05061063e-04, -4.31295339e-04, -4.47731669e-04],\n",
       "        [-5.91669086e-05, -3.16115416e-04,  3.77533433e-05, ...,\n",
       "          7.09407788e-04, -3.07428330e-04, -4.01044235e-04],\n",
       "        ...,\n",
       "        [-1.04003027e-03,  9.24912281e-04,  6.32954529e-04, ...,\n",
       "         -5.97048085e-04, -2.05555225e-05, -4.10445908e-04],\n",
       "        [-1.15864817e-03,  1.24116393e-03,  4.87998390e-04, ...,\n",
       "         -8.69153591e-04,  7.04076319e-06, -5.04843600e-04],\n",
       "        [-1.17444654e-03,  1.18031830e-03,  2.43912596e-04, ...,\n",
       "         -5.39894972e-04, -1.83304568e-04, -3.83758917e-04]],\n",
       "\n",
       "       [[ 1.17548836e-04, -1.14807386e-04, -2.63128841e-05, ...,\n",
       "          5.40828514e-05, -5.65763075e-05, -3.03391222e-04],\n",
       "        [ 2.79202039e-04, -2.21483744e-04, -3.38831596e-05, ...,\n",
       "         -1.96490233e-04, -1.42959994e-04, -4.30256885e-04],\n",
       "        [ 4.96660708e-04, -2.53154023e-04,  5.12696533e-05, ...,\n",
       "         -3.01669090e-04, -2.70302971e-05, -8.03715491e-04],\n",
       "        ...,\n",
       "        [ 6.40289101e-04,  1.19670702e-03,  1.78644899e-03, ...,\n",
       "         -2.50890036e-04,  9.67845335e-05, -2.79574597e-04],\n",
       "        [ 5.97393140e-04,  1.65122654e-03,  2.11065402e-03, ...,\n",
       "         -5.20959671e-04,  3.01854714e-04, -7.96689710e-05],\n",
       "        [ 5.19437017e-04,  2.11199792e-03,  2.36694980e-03, ...,\n",
       "         -8.01326416e-04,  5.34578401e-04,  7.50125764e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4298fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  18882560  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  14345049  \n",
      "=================================================================\n",
      "Total params: 68,582,489\n",
      "Trainable params: 68,582,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914fc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "305/610 [==============>...............] - ETA: 2:05 - loss: 2.0021"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=5)\n",
    "\n",
    "history = model.fit(enc_train, \n",
    "          dec_train,\n",
    "          batch_size=256,\n",
    "          validation_data=(enc_val, dec_val),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "351750b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977/977 - 34s - loss: 1.8281\n",
      "1.8281478881835938\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(enc_val,  dec_val, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_curve(history.epoch, history.history, ['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <END>를 예측하지 않았거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c510cc",
   "metadata": {},
   "source": [
    "### 회고\n",
    "\n",
    "유난히 많은 시간이 걸렸던 노드였던것같다.\n",
    "자연어 처리를 노래 가사로 실험해볼 수 있던게 흥미로웠다.\n",
    "인공지능은 결국 사람을 모방한 것이므로 사람이 언어를 인식하는 방식과 유사해질수록 인공지능의 성능이 높아지지 않을까 하는 생각을 했다.\n",
    "loss 값을 2.2 이하로 떨어뜨리기 위해 epoch, embedding_size, hidden_size 값을 수정했다.\n",
    "epoch를 제외하고 주로 증가를 하는 방향으로 갔던 것 같다. \n",
    "처음엔 epoch 값을 10, 30으로도 해보았지만 시간이 너무 오래 걸려 최소한의 epoch로 loss 값을 2.2 이하로 맞추는 값을 찾기 위해 수정을 가했다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6790f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
